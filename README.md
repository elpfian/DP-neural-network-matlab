# Neural Network v2
School project: Code neural network using Matlab with corrections and additional features over v1. Test with XOR and MNIST.

Run neural_net with the following form:
[weights, biases, accuracy, cost] = neural_net( inputs, targets, nodeLayers, numEpochs, batchSize, split, eta, mu, lambda, transF, costF, wStart, bStart);

Input parameters:
•	inputs = input matrix
•	targets = target matrix
•	nodeLayers = vector of nodes in network (input, hidden and output)
•	numEpochs = # of epochs to complete
•	batchSize = size of mini-batch
•	split = training/test/validation proportions of the form [train, test, val]. Example [.8 .1 .1]
•	eta = learning rate
•	mu = momentum coefficient
•	lambda = L2 regularization coefficient
•	transF = transfer function name given by one of the named m-files. Example @sigma
•	costF = cost function name given by one of the named m-files. Example @x_entropy
•	wStart = optional, cell of initial weights
•	bStart = optional, cell of initial biases

Returned variables:
•	weights = cell array of learned weights
•	biases = cell array of learned biases
•	accuracy = matrix of accuracy for each epoch for train, test and validation sets
•	cost = matrix of cost for each epoch for train, test and validation sets

The function will also print to screen performance for each epoch or every 10th epochs if numEpochs > 100.

### DESCRIPTION

The primary function neural_net:
•	Checks input arguments
•	Calls init_better_network to initialize weight and bias matrices
•	Subsets inputs into randomized train, test and validation sets
•	Contains the main loop over the epochs which:
o	Checks whether the conditions to terminate training have been met 
	Supplied number of epochs completed
	Training accuracy = 100%
	Validation cost has not improved (decreased) in 10 epochs. This number is set by the “stop” variable defined in the code; it is not currently an available input parameter.
o	Calls one_epoch to learn the network weights and biases
o	Calls feed_forward to generate network output for train, test and validation sets
o	Calls cost_acc¬ to generate # correct, cost and accuracy for train, test and validation sets
o	Calls screen_print to print the results of each epoch
o	Updates the accuracy and cost matrices

The function init_better_network:
•	Generates initial weight matrix, bias vector and change in weight and bias for each layer of the network
•	Weights and biases are randomly generated with normal distribution, mean of 0 and standard deviation given by:
o	Weights: 1/sqrt(# samples)
o	Biases: 1

The function one_epoch:
•	Generates shuffled mini-batches for each epoch and iterates over all batches
o	Mini-batches are generated by creating a shuffled index the same length as the training set. The program iteratively selects the next sequence of indices of batch size. Then the inputs and targets are sampled using that batch of indices to create a random batch.
•	Calls feed_forward to generate activations and weighted inputs of the network
•	Calls back_prop_SGD to compute network error, backpropagate and perform stochastic gradient descent after each batch
•	Returns the learned weights, biases and change in weights/biases from last update at the end of the epoch

The function feed_forward:
•	Propagates the input matrix through the network with different functions at the output and hidden nodes depending on the supplied transfer and cost functions
•	Calls transfer functions
•	Returns the activations and weighted inputs of the network

The function back_prop_SGD:
•	Calculates appropriate output error using the cost function name
•	Backpropagates error through the network
•	Uses SGD to update weights and biases
•	Stores change in weights and biases from previous update
•	Returns updated weights, biases and change in weights/biases

The function cost_acc:
•	Returns # correct, cost and accuracy for a given output and target

The function screen_print:
•	Prints formatted network performance for the given epoch

Cost functions log_like, quad_cost and x_entropy:
•	Calculate the cost between given outputs and targets

Transfer functions relu, sigma, softmax, and tan_h:
•	Apply given function or its derivative (based on derivative flag variable) to a matrix
